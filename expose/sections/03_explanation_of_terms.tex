\section{Explanation of Terms}

\subsection{Neural Architecture Search (NAS)}

Traditionally, neural network architectures are designed by a team of domain and Deep Learning experts for a specific Deep Learning application. In NAS, the architecture is automatically searched by continuously evaluating the performance of candidate architectures and updating the architectures with high performance for a given task. NAS methods differ in their \textit{search space} for potential candidate architectures, the \textit{search strategy} they employ to find an optimal architecture, and the \textit{performance estimation strategy} used by the search strategy to make decisions on which candidates to favour or explore next.

\subsection{Federated Learning}

FL is a machine learning approach in which multiple \textit{clients} collaboratively train a shared model while keeping training data local to the clients. A central \textit{server} coordinates \textit{communication rounds} with the clients, wherein each client trains the model for several epochs locally and finally sends gradient updates to the server for aggregation into the shared model.

\subsection{Adaptation technique}

An adaptation technique is a modification to a centralised NAS method that is explicitly motivated by making the NAS method feasible for FL. We refer to the example from the problem statement: The FedNAS method \textit{FedorAS}~\cite{fedoras_2022} makes use of the centralised NAS method \textit{SPOS}~\cite{spos_2020}, but runs into the following challenge caused by the fact that centralised NAS can assume worker nodes are connected via low latency, high-bandwidth links, whereas clients in FL are not: sending the parameters of the entire supernet used in SPOS to each client would take an infeasible amount of time. Instead, \textit{FedorAS} adapts \textit{SPOS} such that only a small, sampled subspace of the supernet is sent to each client for training and evaluation.